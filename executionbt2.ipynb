{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YApzfqe0x1vw"
      },
      "source": [
        "#1 Answer\n",
        "\n",
        "Choosing a model for temporary market impact, $g_t(x)$, is critical for any effective order execution system. A simple linear model, $g_t(x)=beta_tx$, is a significant oversimplification. It incorrectly assumes that each share in an order has the same marginal cost. This is not how it works. The structure of a limit order book, with its discrete price levels and finite liquidity, means that larger orders must go through multiple levels in the order book consuming liquidity at progressively worse prices. This results in a non-linear concave impact function where impact grows more slowly than order size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo57czLZzDw8"
      },
      "source": [
        "Analysis of the three tickers(SOUN,FROG,CRWV) demonstrates that no single fixed formula can reliably model market impact across different stocks. The best approach is an empirical journey, starting with simple models and progressing to more powerful, data driven methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEtesh17zh18"
      },
      "source": [
        "# Square Root Model and Its Limitations\n",
        "A strong theoretical starting point is the Square Root Impact Model, g_t(x)=\n",
        "sigma_t*sqrt(x).However, when validated against the three tickers, its limitations became clear, as measured by the R-squared value\n",
        "\n",
        "\n",
        "* SOUN(High-Liquidity): The model failed completely. The analysis showed consistently negative R-squared values, meaning its predictions were worse than a simple average.\n",
        "* FROG(Medium-Liquidity): The model was inconsistent. It achieved a positive average R-squared of 0.21 but the fit was highly volatile.\n",
        "\n",
        "\n",
        "* CRWV (Illiquid): The model was unreliable, with a volatile fit and an average R-squared of approximately 0.33.\n",
        "\n",
        "While a Piecewise Model offers an improvement by penalizing large orders more heavily, it still relies on the rigid square root assumption.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq3qo5nh0jEO"
      },
      "source": [
        "# The Superior Solution: A Gradient Boosting Model\n",
        "This machine learning model learns the complex patterns of market impact directly from the data without being confined to a fixed formula.ts superiority was demonstrated by its exceptional performance across all three stocks when using all 10 levels of order book data as features:\n",
        "\n",
        "\n",
        "*   $SOUN(High Liquidity): R^2 = 0.5043$\n",
        "*   $FROG(Medium Liquidity): R^2= 0.6022$\n",
        "\n",
        "\n",
        "*   $CRWV(Illiquid):: R^2= 0.4847$\n",
        "\n",
        "The Gradient Boosting model succeeded because it leverages a rich set of features including the spread, liquidity imbalance, and the depth of the entire order book to make highly accurate, context aware predictions for each specific stock.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TdrnOFcwEyyB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Modeling\n",
        "USE_LGBM = True\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    USE_LGBM = False\n",
        "    from sklearn.ensemble import GradientBoostingRegressor as FallbackGBR\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# -------------------------\n",
        "# Utility: Seeding\n",
        "# -------------------------\n",
        "def set_seed(seed=22):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    try:\n",
        "        import torch\n",
        "        torch.manual_seed(seed)\n",
        "    except Exception:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_true_impact(row, order_size):\n",
        "    bid0 = row.get(\"bid_px_00\")\n",
        "    ask0 = row.get(\"ask_px_00\")\n",
        "    if pd.isna(bid0) or pd.isna(ask0):\n",
        "        return np.nan\n",
        "\n",
        "    mid = (bid0 + ask0) / 2.0\n",
        "    remaining, total_cost, filled = order_size, 0.0, 0.0\n",
        "\n",
        "    for i in range(10):\n",
        "        price = row.get(f\"ask_px_{i:02d}\")\n",
        "        size = row.get(f\"ask_sz_{i:02d}\")\n",
        "        if price is None or size is None or pd.isna(price) or pd.isna(size) or size <= 0:\n",
        "            continue\n",
        "\n",
        "        fill = min(remaining, size)\n",
        "        total_cost += fill * price\n",
        "        filled += fill\n",
        "        remaining -= fill\n",
        "        if remaining <= 0:\n",
        "            break\n",
        "\n",
        "    if remaining > 0 or filled == 0:\n",
        "        return np.nan\n",
        "\n",
        "    exec_avg = total_cost / filled\n",
        "    return (exec_avg - mid) / mid\n"
      ],
      "metadata": {
        "id": "BV8yHto5lNXc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_timestamp_column(df):\n",
        "    candidates = [\"timestamp\", \"time\", \"datetime\", \"date\", \"ts\"]\n",
        "    for c in df.columns:\n",
        "        if c.lower() in candidates:\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        try:\n",
        "            pd.to_datetime(df[c].head(5))\n",
        "            return c\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None"
      ],
      "metadata": {
        "id": "wyoa0SgWlRiP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_feature_dataset(df, num_snapshots=10000, test_sizes=None,\n",
        "                           ts_col=None, normalize=False, random_state=42):\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    df = df.reset_index(drop=False).rename(columns={\"index\": \"snapshot_id\"})\n",
        "    if ts_col is not None and ts_col in df.columns:\n",
        "        try:\n",
        "            df[ts_col] = pd.to_datetime(df[ts_col])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if test_sizes is None:\n",
        "        test_sizes = [50, 100, 200, 500, 1_000, 2_000, 5_000, 10_000, 100_000]\n",
        "\n",
        "    n = len(df)\n",
        "    k = min(num_snapshots, n)\n",
        "    sample_indices = rng.choice(n, size=k, replace=False)\n",
        "\n",
        "    rows = []\n",
        "    for idx in sample_indices:\n",
        "        row = df.iloc[idx]\n",
        "        bid0, ask0 = row.get(\"bid_px_00\"), row.get(\"ask_px_00\")\n",
        "        if pd.isna(bid0) or pd.isna(ask0):\n",
        "            continue\n",
        "\n",
        "        mid = (bid0 + ask0) / 2.0\n",
        "        ask_liq = sum(row.get(f\"ask_sz_{i:02d}\", 0) for i in range(10))\n",
        "        bid_liq = sum(row.get(f\"bid_sz_{i:02d}\", 0) for i in range(10))\n",
        "        depth = ask_liq + bid_liq + 1e-9\n",
        "\n",
        "        for size in test_sizes:\n",
        "            impact = calculate_true_impact(row, size)\n",
        "            if pd.isna(impact):\n",
        "                continue\n",
        "\n",
        "            feat = {\n",
        "                \"snapshot_id\": row[\"snapshot_id\"],\n",
        "                \"order_size\": size,\n",
        "                \"target_impact\": impact,\n",
        "                \"mid\": mid,\n",
        "                \"spread\": ask0 - bid0,\n",
        "                \"liq_imbalance\": (bid_liq - ask_liq) / depth,\n",
        "            }\n",
        "            if ts_col is not None and ts_col in df.columns:\n",
        "                feat[\"timestamp\"] = row[ts_col]\n",
        "\n",
        "            for i in range(10):\n",
        "                ap = row.get(f\"ask_px_{i:02d}\")\n",
        "                asz = row.get(f\"ask_sz_{i:02d}\")\n",
        "                bp = row.get(f\"bid_px_{i:02d}\")\n",
        "                bsz = row.get(f\"bid_sz_{i:02d}\")\n",
        "\n",
        "                if normalize:\n",
        "                    feat[f\"ask_px_{i:02d}\"] = (ap / mid) if pd.notna(ap) else np.nan\n",
        "                    feat[f\"bid_px_{i:02d}\"] = (bp / mid) if pd.notna(bp) else np.nan\n",
        "                    feat[f\"ask_sz_{i:02d}\"] = (asz / depth) if pd.notna(asz) else np.nan\n",
        "                    feat[f\"bid_sz_{i:02d}\"] = (bsz / depth) if pd.notna(bsz) else np.nan\n",
        "                else:\n",
        "                    feat[f\"ask_px_{i:02d}\"] = ap\n",
        "                    feat[f\"ask_sz_{i:02d}\"] = asz\n",
        "                    feat[f\"bid_px_{i:02d}\"] = bp\n",
        "                    feat[f\"bid_sz_{i:02d}\"] = bsz\n",
        "\n",
        "            rows.append(feat)\n",
        "\n",
        "    feat_df = pd.DataFrame(rows).dropna()\n",
        "    return feat_df"
      ],
      "metadata": {
        "id": "-GRjzHttlVCe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 4) Splits\n",
        "# -------------------------\n",
        "def group_aware_split(feature_df, test_size=0.2, random_state=42):\n",
        "    assert \"snapshot_id\" in feature_df.columns\n",
        "    X = feature_df.drop(columns=[\"target_impact\"])\n",
        "    y = feature_df[\"target_impact\"]\n",
        "    groups = feature_df[\"snapshot_id\"]\n",
        "\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    tr_idx, te_idx = next(gss.split(X, y, groups))\n",
        "    return X.iloc[tr_idx].copy(), X.iloc[te_idx].copy(), y.iloc[tr_idx].copy(), y.iloc[te_idx].copy()\n",
        "\n",
        "\n",
        "def time_based_split(feature_df, test_frac=0.2):\n",
        "    assert \"timestamp\" in feature_df.columns\n",
        "    df_sorted = feature_df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    cutoff = int((1.0 - test_frac) * len(df_sorted))\n",
        "    train_df = df_sorted.iloc[:cutoff].copy()\n",
        "    test_df = df_sorted.iloc[cutoff:].copy()\n",
        "\n",
        "    X_train = train_df.drop(columns=[\"target_impact\"])\n",
        "    y_train = train_df[\"target_impact\"]\n",
        "    X_test = test_df.drop(columns=[\"target_impact\"])\n",
        "    y_test = test_df[\"target_impact\"]\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "xD4IuNOGlYHS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 5) Training / Evaluation\n",
        "# -------------------------\n",
        "def train_and_eval(feature_df, prefer_time_split=True, test_size=0.2,\n",
        "                   random_state=22, monotone_on_order_size=False):\n",
        "\n",
        "    if prefer_time_split and \"timestamp\" in feature_df.columns:\n",
        "        X_train, X_test, y_train, y_test = time_based_split(feature_df, test_frac=test_size)\n",
        "        split_used = \"time-based\"\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = group_aware_split(feature_df, test_size=test_size, random_state=random_state)\n",
        "        split_used = \"group-aware\"\n",
        "\n",
        "    # Drop non-model columns\n",
        "    for col in [\"snapshot_id\", \"timestamp\"]:\n",
        "        if col in X_train.columns:\n",
        "            X_train = X_train.drop(columns=[col])\n",
        "            X_test = X_test.drop(columns=[col])\n",
        "\n",
        "    # Train\n",
        "    if USE_LGBM:\n",
        "        if monotone_on_order_size and \"order_size\" in X_train.columns:\n",
        "            mono = [0] * X_train.shape[1]\n",
        "            mono[X_train.columns.get_loc(\"order_size\")] = 1\n",
        "            model = lgb.LGBMRegressor(random_state=random_state, monotone_constraints=mono)\n",
        "        else:\n",
        "            model = lgb.LGBMRegressor(random_state=random_state)\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        pred = model.predict(X_test)\n",
        "        fi = pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "    else:\n",
        "        model = FallbackGBR(random_state=random_state)\n",
        "        model.fit(X_train, y_train)\n",
        "        pred = model.predict(X_test)\n",
        "        fi = None\n",
        "\n",
        "    r2 = r2_score(y_test, pred)\n",
        "    mae = mean_absolute_error(y_test, pred)\n",
        "\n",
        "    return {\n",
        "        \"split_used\": split_used,\n",
        "        \"X_train_shape\": X_train.shape,\n",
        "        \"X_test_shape\": X_test.shape,\n",
        "        \"R2\": r2,\n",
        "        \"MAE\": mae,\n",
        "        \"feature_importance\": fi\n",
        "    }"
      ],
      "metadata": {
        "id": "TuwnBYHrlbry"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    \"/content/CRWV_2025-04-03 00:00:00+00:00.csv\",\n",
        "    \"/content/FROG_2025-04-03 00:00:00+00:00.csv\",\n",
        "    \"/content/SOUN_2025-04-03 00:00:00+00:00.csv\"   # replace with actual filename/path\n",
        "]\n",
        "\n",
        "for f in files:\n",
        "    print(f\"\\n=== Running on {os.path.basename(f)} ===\")\n",
        "    df = pd.read_csv(f)\n",
        "\n",
        "    # detect timestamp column if present\n",
        "    ts_col = find_timestamp_column(df)\n",
        "\n",
        "    # build feature dataset\n",
        "    feat_df = create_feature_dataset(df, num_snapshots=5000, ts_col=ts_col)\n",
        "\n",
        "    if len(feat_df) < 200:\n",
        "        print(f\"Too few rows after feature creation: {len(feat_df)}\")\n",
        "        continue\n",
        "\n",
        "    # train and evaluate\n",
        "    results = train_and_eval(feat_df, prefer_time_split=True)\n",
        "    print(json.dumps({\n",
        "        \"split_used\": results[\"split_used\"],\n",
        "        \"R2\": results[\"R2\"],\n",
        "        \"MAE\": results[\"MAE\"],\n",
        "        \"top_features\": results[\"feature_importance\"].head(10).to_dict()\n",
        "                           if results[\"feature_importance\"] is not None else None\n",
        "    }, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D011oqt2mAP6",
        "outputId": "7cd8a5dd-4cdb-4efc-9648-bcd6db2e33d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running on CRWV_2025-04-03 00:00:00+00:00.csv ===\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003634 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 10493\n",
            "[LightGBM] [Info] Number of data points in the train set: 22190, number of used features: 44\n",
            "[LightGBM] [Info] Start training from score 0.002281\n",
            "{\n",
            "  \"split_used\": \"time-based\",\n",
            "  \"R2\": 0.4846743637899744,\n",
            "  \"MAE\": 0.0005728374570291515,\n",
            "  \"top_features\": {\n",
            "    \"order_size\": 421,\n",
            "    \"ask_sz_00\": 316,\n",
            "    \"spread\": 184,\n",
            "    \"ask_px_00\": 126,\n",
            "    \"ask_sz_01\": 126,\n",
            "    \"ask_px_09\": 124,\n",
            "    \"ask_sz_02\": 105,\n",
            "    \"ask_sz_03\": 97,\n",
            "    \"mid\": 93,\n",
            "    \"ask_sz_04\": 89\n",
            "  }\n",
            "}\n",
            "\n",
            "=== Running on FROG_2025-04-03 00:00:00+00:00.csv ===\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003294 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6069\n",
            "[LightGBM] [Info] Number of data points in the train set: 19737, number of used features: 44\n",
            "[LightGBM] [Info] Start training from score 0.001892\n",
            "{\n",
            "  \"split_used\": \"time-based\",\n",
            "  \"R2\": 0.6021510003884699,\n",
            "  \"MAE\": 0.0002657705888902536,\n",
            "  \"top_features\": {\n",
            "    \"order_size\": 456,\n",
            "    \"spread\": 348,\n",
            "    \"ask_sz_00\": 320,\n",
            "    \"ask_sz_01\": 184,\n",
            "    \"ask_sz_02\": 124,\n",
            "    \"ask_sz_03\": 103,\n",
            "    \"mid\": 95,\n",
            "    \"ask_sz_04\": 95,\n",
            "    \"ask_sz_05\": 77,\n",
            "    \"ask_px_00\": 75\n",
            "  }\n",
            "}\n",
            "\n",
            "=== Running on SOUN_2025-04-03 00:00:00+00:00.csv ===\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005289 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6576\n",
            "[LightGBM] [Info] Number of data points in the train set: 32015, number of used features: 44\n",
            "[LightGBM] [Info] Start training from score 0.001116\n",
            "{\n",
            "  \"split_used\": \"time-based\",\n",
            "  \"R2\": 0.5043178747301649,\n",
            "  \"MAE\": 5.2948411456542335e-05,\n",
            "  \"top_features\": {\n",
            "    \"ask_sz_00\": 732,\n",
            "    \"order_size\": 672,\n",
            "    \"ask_sz_01\": 453,\n",
            "    \"spread\": 279,\n",
            "    \"ask_sz_02\": 196,\n",
            "    \"ask_sz_03\": 107,\n",
            "    \"mid\": 89,\n",
            "    \"ask_sz_04\": 55,\n",
            "    \"liq_imbalance\": 50,\n",
            "    \"bid_sz_02\": 44\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJn0UJAU4ZqQ"
      },
      "source": [
        "# 2 Answer\n",
        " The goal is to execute a total of S shares over N discrete trading periods (390 one minute intervals) in a way that minimizes the total cost from temporary market impact. We need to find an allocation vector X that defines the number of shares to trade in each period i.\n",
        "The total cost, C(X), is the sum of the costs of each individual trade. The cost of a single trade is its size multiplied by its impact.the most accurate impact forecast is provided by our trained Gradient Boosting Model, GBM_predict. This model serves as our cost function.\n",
        "\n",
        "$\\min_{C(X)} = \\sum_{i=1}^{N} \\text{Cost}_i(x_i) = \\sum_{i=1}^{N} x_i \\cdot \\text{GBM\\_predict}(\\text{features}_i, x_i)$\n",
        "\n",
        "Gradient Boosting model is not interpretable we cannot use simple calculus to find the perfect answer all at once. Instead, we use a smart, step by step algorithm that makes good decisions in real time.algorithm works by intelligently adjusting its trading speed based on historic conditions which the model should be trained. At any given moment, it knows the simple average number of shares it needs to trade per minute to finish the order on time the baseline rate(S/N). It then uses our trained ML model to predict the cost of trading right now. If the model predicts that the current impact is lower than the day's average (meaning it's a cheap time to trade), it will trade more than the baseline rate. Conversely, if the model predicts a high impact (an expensive time), it will trade less, saving shares to execute later when conditions might be better. This simple logic of buy more when it's cheap, less when it's expensive allows the algorithm to naturally reduce costs, while safety checks ensure the full order is completed by the end of the day.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}